{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Arnesh\n",
      "[nltk_data]     Bhattacharjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Arnesh\n",
      "[nltk_data]     Bhattacharjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn for preprocessing and machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics  import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras for neural networks\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics  import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(3,5))\n",
    "sns.countplot(raw_df['airline_sentiment'], order =raw_df.airline_sentiment.value_counts().index,palette= 'plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-81-f2938b92463f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-81-f2938b92463f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    g = sns.FacetGrid(raw_df, col=”airline”, col_wrap=3, height=5, aspect =0.7) g = g.map(sns.countplot, “airline_sentiment”,order =raw_df[airline_sentiment].value_counts().index, palette=’plasma’) plt.show()\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "g = sns.FacetGrid(raw_df, col=”airline”, col_wrap=3, height=5, aspect =0.7) g = g.map(sns.countplot, “airline_sentiment”,order =raw_df[airline_sentiment].value_counts().index, palette=’plasma’) plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                            0\n",
       "airline_sentiment                   0\n",
       "airline_sentiment_confidence        0\n",
       "negativereason                   5462\n",
       "negativereason_confidence        4118\n",
       "airline                             0\n",
       "airline_sentiment_gold          14600\n",
       "name                                0\n",
       "negativereason_gold             14608\n",
       "retweet_count                       0\n",
       "text                                0\n",
       "tweet_coord                     13621\n",
       "tweet_created                       0\n",
       "tweet_location                   4733\n",
       "user_timezone                    4820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tensorflow.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    '''\n",
    "    Easily performs all the standard preprocessing steps\n",
    "    like removing stopwords, stemming, etc.\n",
    "    Only input that you need to provide is the dataframe and column name for the tweets\n",
    "    '''\n",
    "    def __init__(self, df, column_name):\n",
    "        self.data = df\n",
    "        self.conversations = list(self.data[column_name])\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.preprocessed = []\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        '''\n",
    "        Splits up words and makes a list of all words in the tweet\n",
    "        '''\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def remove_stopwords(self, sentence):\n",
    "        '''Removes stopwords like 'a', 'the', 'and', etc.'''\n",
    "        filtered_sentence = []\n",
    "        for w in sentence:\n",
    "            if w not in self.stopwords and len(w) > 1 and w[:2] != '//' and w != 'https':\n",
    "                filtered_sentence.append(w)\n",
    "        return filtered_sentence\n",
    "\n",
    "    def stem(self, sentence):\n",
    "        '''\n",
    "        Stems certain words to their root form.\n",
    "        For example, words like 'computer', 'computation'\n",
    "        all get truncated to 'comput'\n",
    "        '''\n",
    "        return [self.stemmer.stem(word) for word in sentence]\n",
    "\n",
    "    def join_to_string(self, sentence):\n",
    "        '''\n",
    "        Joins the tokenized words to one string.\n",
    "        '''\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def full_preprocess(self, n_rows=None):\n",
    "        '''\n",
    "        Preprocess a selected number of rows and\n",
    "        connects them back to strings\n",
    "        '''\n",
    "        # If nothing is given do it for the whole dataset\n",
    "        if n_rows == None:\n",
    "            n_rows = len(self.data)\n",
    "\n",
    "        # Perform preprocessing\n",
    "        for i in range(n_rows):\n",
    "            tweet = self.conversations[i]\n",
    "            tokenized = self.tokenize(tweet)\n",
    "            cleaned = self.remove_stopwords(tokenized)\n",
    "            stemmed = self.stem(cleaned)\n",
    "            joined = self.join_to_string(stemmed)\n",
    "            self.preprocessed.append(joined)\n",
    "        return self.preprocessed\n",
    "    \n",
    "    def strip_html(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def remove_numbers(text):\n",
    "       text = re.sub(r'\\d+', '', text)\n",
    "       return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-10529ad9f6ee>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = preprocessor.full_preprocess()\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(df, 'text')\n",
    "df['cleaned_text'] = preprocessor.full_preprocess()\n",
    "df = shuffle(df, random_state=seed)\n",
    "# Create test set\n",
    "test_set = df[:1000]\n",
    "\n",
    "# Create train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['cleaned_text'][1000:],\n",
    "                                                      df['airline_sentiment'][1000:],\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=seed)\n",
    "\n",
    "# Get sentiment labels for test set\n",
    "y_test = test_set['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(test_set['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator so we can easily feed batches of data to the neural network\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0]/batch_size\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "# Onehot encoding of target variable\n",
    "# Negative = [1,0,0], Neutral = [0,1,0], Positive = [0,0,1]\n",
    "\n",
    "# Initialize sklearn's one-hot encoder class\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# One hot encoding for training set\n",
    "integer_encoded_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "onehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "# One hot encoding for validation set\n",
    "integer_encoded_val = np.array(y_val).reshape(len(y_val), 1)\n",
    "onehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)\n",
    "\n",
    "# Neural network architecture\n",
    "initializer = tensorflow.keras.initializers.he_normal(seed=seed)\n",
    "activation = tensorflow.keras.activations.elu\n",
    "optimizer = tensorflow.keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "\n",
    "# Build model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation=activation, kernel_initializer=initializer, input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer=initializer))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnesh Bhattacharjee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "680/682 [============================>.] - ETA: 0s - loss: 0.6389 - accuracy: 0.5860"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1] = [0,7387] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-c12b6ea17e81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Fit the model using the batch_generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m hist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True),\n\u001b[0m\u001b[0;32m      7\u001b[0m                            \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_encoded_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                            steps_per_epoch=X_train.shape[0]/batch_size, callbacks=[es])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1845\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1847\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1848\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1116\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_eval_data_handler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1118\u001b[1;33m             self._eval_data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1119\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \"\"\"\n\u001b[1;32m--> 691\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   3155\u001b[0m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3157\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3158\u001b[0m     self._structure = nest.map_structure(\n\u001b[0;32m   3159\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[1;34m(element_spec, element)\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m   \u001b[1;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m   return _to_tensor_list_helper(\n\u001b[0m\u001b[0;32m    365\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0;32m    366\u001b[0m           component), element_spec, element)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[1;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m   return functools.reduce(\n\u001b[0m\u001b[0;32m    340\u001b[0m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[1;34m(state, value)\u001b[0m\n\u001b[0;32m    335\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m   return functools.reduce(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(state, spec, component)\u001b[0m\n\u001b[0;32m    363\u001b[0m   \u001b[1;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m--> 365\u001b[1;33m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0m\u001b[0;32m    366\u001b[0m           component), element_spec, element)\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\u001b[0m in \u001b[0;36m_to_batched_tensor_list\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    367\u001b[0m       raise ValueError(\n\u001b[0;32m    368\u001b[0m           \"Unbatching a sparse tensor is only supported for rank >= 1\")\n\u001b[1;32m--> 369\u001b[1;33m     return [gen_sparse_ops.serialize_many_sparse(\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         out_type=dtypes.variant)]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\u001b[0m in \u001b[0;36mserialize_many_sparse\u001b[1;34m(sparse_indices, sparse_values, sparse_shape, out_type, name)\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[1] = [0,7387] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "\n",
    "# Fit the model using the batch_generator\n",
    "hist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True),\n",
    "                           epochs=epochs, validation_data=(X_val, onehot_encoded_val),\n",
    "                           steps_per_epoch=X_train.shape[0]/batch_size, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Arnesh\n",
      "[nltk_data]     Bhattacharjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re                                     \n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arnesh bhattacharjee\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000)                # Keep only 1000 features as number of features will increase the processing time.\n",
    "data_features = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "data_features = data_features.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     text airline_sentiment  \\\n",
       "11135  @USAirways we bought our tickets months ago. H...          negative   \n",
       "3783   @united why is my flight from mke to ord getti...          negative   \n",
       "13332  @AmericanAir My boyfriend was supposed to be h...          negative   \n",
       "11639                @USAirways sits on a throne of lies          negative   \n",
       "4496   @SouthwestAir it's just the principle - it's h...          negative   \n",
       "...                                                  ...               ...   \n",
       "11468  @USAirways why would a flight arrive Late Flig...          negative   \n",
       "7221   @JetBlue I tried! Had me running from curb-sid...          negative   \n",
       "1318   @united Stopped flying @united 1 yr ago bc of ...          negative   \n",
       "8915   @JetBlue's new CEO seeks the right balance to ...           neutral   \n",
       "11055          @USAirways we got our bags today. Thanks!           neutral   \n",
       "\n",
       "                                            cleaned_text  \n",
       "11135  usairway bought ticket month ago had seat toge...  \n",
       "3783   unit flight mke ord get delay our gate attend ...  \n",
       "13332  americanair my boyfriend suppos home saturday ...  \n",
       "11639                            usairway sit throne lie  \n",
       "4496   southwestair 's principl 's hard get mug amp u...  \n",
       "...                                                  ...  \n",
       "11468  usairway would flight arriv late flight leav 6...  \n",
       "7221   jetblu tri had run curb-sid self-check amp was...  \n",
       "1318   unit stop fli unit yr ago bc aggress polici ca...  \n",
       "8915   jetblu 's new ceo seek right balanc pleas pass...  \n",
       "11055                       usairway got bag today thank  \n",
       "\n",
       "[14640 rows x 3 columns]>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-ee218ac8105d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# alt. method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# alt. method\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-87-0a2a836ff1c4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-87-0a2a836ff1c4>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    ('norm', Normalizer()),Z\"\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "twitter_sentiment = Pipeline([('CVec', CountVectorizer(CountVectorizer(stop_words='english'))),\n",
    "                     ('Tfidf', TfidfTransformer()),\n",
    "                      ('norm', Normalizer()),Z\"\n",
    "                    ('tSVD', TruncatedSVD(n_components=100)),\n",
    "                     ('lgb', LGBMClassifier(n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our optimization, it is straightforward to select the hyperparameter configuration that yielded the highest accuracy, lowest loss, or whatever performance you were seeking to optimize. Here we keep the optimization problem rather simple: we only search epoch, batch_size, and dropout. Select the neural net architecture for now and fine tune it. Note if we decide we'd like to revisit those architectures in the future, all we'll have to do is view those and we'll be able to reproduce them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
